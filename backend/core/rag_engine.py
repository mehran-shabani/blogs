"""
Ù…ÙˆØªÙˆØ± RAG Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
"""
from typing import List, Dict, Any, Optional
import re
from hazm import Normalizer, word_tokenize
from db.vector_store import vector_store
from core.openai_client import openai_client
from core.firecrawl_client import firecrawl_client


class RAGEngine:
    """Ù…ÙˆØªÙˆØ± RAG Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³ÙˆØ§Ù„Ø§Øª ÙØ§Ø±Ø³ÛŒ"""
    
    def __init__(self):
        self.normalizer = Normalizer()
        self.vector_store = vector_store
        self.openai_client = openai_client
        self.firecrawl_client = firecrawl_client
    
    def normalize_text(self, text: str) -> str:
        """Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
        # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        text = re.sub(r'\s+', ' ', text)
        text = self.normalizer.normalize(text)
        return text.strip()
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©ØªØ±"""
        words = word_tokenize(text)
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    def process_query(
        self,
        query: str,
        use_web_search: bool = False,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø³Ø´ Ùˆ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        
        Pipeline:
        1. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø³Ø´
        2. Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ø± Vector Store
        3. Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²ØŒ Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ø¯Ø± ÙˆØ¨
        4. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ RAG
        """
        
        # 1. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø³Ø´
        normalized_query = self.normalize_text(query)
        print(f"ğŸ” Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù¾Ø±Ø³Ø´: {normalized_query}")
        
        # 2. Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ù…Ø¹Ù†Ø§ÛŒÛŒ
        search_results = self.vector_store.search(
            query=normalized_query,
            top_k=top_k,
            score_threshold=0.5
        )
        
        print(f"ğŸ“š ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬ ÛŒØ§ÙØªâ€ŒØ´Ø¯Ù‡: {len(search_results)}")
        
        # 3. Ø§Ú¯Ø± Ù†ØªÛŒØ¬Ù‡ Ú©Ø§ÙÛŒ Ù†Ø¨ÙˆØ¯ Ùˆ Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ ÙˆØ¨ ÙØ¹Ø§Ù„ Ø¨Ø§Ø´Ø¯
        if len(search_results) < 2 and use_web_search:
            print("ğŸŒ Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ø¯Ø± ÙˆØ¨...")
            web_results = self.search_web(normalized_query)
            
            # Ø§ÙØ²ÙˆØ¯Ù† Ù†ØªØ§ÛŒØ¬ ÙˆØ¨ Ø¨Ù‡ Vector Store
            if web_results:
                texts = [r['content'] for r in web_results]
                metadatas = [{'url': r['url'], 'title': r['title']} for r in web_results]
                self.vector_store.add_documents(texts, metadatas)
                
                # Ø¬Ø³Øªâ€ŒÙˆØ¬ÙˆÛŒ Ù…Ø¬Ø¯Ø¯
                search_results = self.vector_store.search(
                    query=normalized_query,
                    top_k=top_k,
                    score_threshold=0.3
                )
        
        # 4. ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®
        if not search_results:
            return {
                "answer": "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§ØªÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¨Ù‡ Ø§ÛŒÙ† Ø³Ø¤Ø§Ù„ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø¤Ø§Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ù¾Ø±Ø³ÛŒØ¯ ÛŒØ§ Ø§Ø¨ØªØ¯Ø§ URLÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯.",
                "sources": [],
                "query": query,
                "search_results": []
            }
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ context Ùˆ Ù…Ù†Ø§Ø¨Ø¹
        context = "\n\n".join([
            f"Ù…ØªÙ† {i+1}:\n{result['text'][:1000]}"
            for i, result in enumerate(search_results)
        ])
        
        sources = []
        for result in search_results:
            metadata = result.get('metadata', {})
            source = metadata.get('url', '') or metadata.get('title', 'Ù…Ù†Ø¨Ø¹ Ù†Ø§Ø´Ù†Ø§Ø³')
            if source and source not in sources:
                sources.append(source)
        
        # ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® Ø¨Ø§ OpenAI
        rag_response = self.openai_client.generate_rag_response(
            query=normalized_query,
            context=context,
            sources=sources
        )
        
        return {
            **rag_response,
            "search_results": search_results[:3]  # ÙÙ‚Ø· 3 Ù†ØªÛŒØ¬Ù‡ Ø¨Ø±ØªØ±
        }
    
    def search_web(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """Ø¬Ø³Øªâ€ŒÙˆØ¬Ùˆ Ø¯Ø± ÙˆØ¨ Ø¨Ø§ Firecrawl"""
        return self.firecrawl_client.search_web(query, max_results)
    
    def ingest_url(self, url: str) -> Dict[str, Any]:
        """Ø§ÙØ²ÙˆØ¯Ù† URL Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡"""
        print(f"ğŸ“¥ Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ: {url}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Firecrawl
        scraped_data = self.firecrawl_client.scrape_url(url)
        
        if not scraped_data:
            return {
                "success": False,
                "message": "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù…Ø­ØªÙˆØ§ÛŒ URL"
            }
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªÙ‚Ø³ÛŒÙ… Ù…Ø­ØªÙˆØ§
        content = scraped_data.get('content', '')
        normalized_content = self.normalize_text(content)
        chunks = self.chunk_text(normalized_content)
        
        print(f"ğŸ“ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯Ø´Ø¯Ù‡: {len(chunks)}")
        
        # Ø§ÙØ²ÙˆØ¯Ù† Ø¨Ù‡ Vector Store
        metadatas = [{
            'url': url,
            'title': scraped_data.get('title', ''),
            'chunk_index': i
        } for i in range(len(chunks))]
        
        self.vector_store.add_documents(chunks, metadatas)
        
        return {
            "success": True,
            "message": f"âœ… {len(chunks)} ØªÚ©Ù‡ Ø§Ø² Ù…Ø­ØªÙˆØ§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯",
            "url": url,
            "title": scraped_data.get('title', ''),
            "chunks_count": len(chunks)
        }


# Ù†Ù…ÙˆÙ†Ù‡ Ø³Ø±Ø§Ø³Ø±ÛŒ
rag_engine = RAGEngine()
