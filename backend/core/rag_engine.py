"""
????? RAG ???? ?????? ??????? ? ????? ????
"""
from typing import List, Dict, Any, Optional
import re
from hazm import Normalizer, word_tokenize
from db.vector_store import vector_store
from core.openai_client import openai_client
from core.firecrawl_client import firecrawl_client


class RAGEngine:
    """????? RAG ???? ?????? ?????? ?????"""
    
    def __init__(self):
        self.normalizer = Normalizer()
        self.vector_store = vector_store
        self.openai_client = openai_client
        self.firecrawl_client = firecrawl_client
    
    def normalize_text(self, text: str) -> str:
        """?????????? ??? ?????"""
        # ??? ?????????? ?????
        text = re.sub(r'\s+', ' ', text)
        text = self.normalizer.normalize(text)
        return text.strip()
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """????? ??? ?? ??????? ??????"""
        words = word_tokenize(text)
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            if chunk:
                chunks.append(chunk)
        
        return chunks
    
    def process_query(
        self,
        query: str,
        use_web_search: bool = False,
        top_k: int = 5
    ) -> Dict[str, Any]:
        """
        ?????? ???? ? ????? ????
        
        Pipeline:
        1. ?????????? ????
        2. ???????? ?????? ?? Vector Store
        3. ?? ???? ????? ??????? ?? ??
        4. ????? ???? ?? RAG
        """
        
        # 1. ?????????? ????
        normalized_query = self.normalize_text(query)
        print(f"?? ?????? ????: {normalized_query}")
        
        # 2. ???????? ??????
        search_results = self.vector_store.search(
            query=normalized_query,
            top_k=top_k,
            score_threshold=0.5
        )
        
        print(f"?? ????? ????? ????????: {len(search_results)}")
        
        # 3. ??? ????? ???? ???? ? ???????? ?? ???? ????
        if len(search_results) < 2 and use_web_search:
            print("?? ??????? ?? ??...")
            web_results = self.search_web(normalized_query)
            
            # ?????? ????? ?? ?? Vector Store
            if web_results:
                texts = [r['content'] for r in web_results]
                metadatas = [{'url': r['url'], 'title': r['title']} for r in web_results]
                self.vector_store.add_documents(texts, metadatas)
                
                # ???????? ????
                search_results = self.vector_store.search(
                    query=normalized_query,
                    top_k=top_k,
                    score_threshold=0.3
                )
        
        # 4. ????? ????
        if not search_results:
            return {
                "answer": "???????? ???????? ???? ???? ?? ??? ???? ???? ???. ????? ???? ????? ?????? ?? ????? URL??? ????? ?? ????? ????.",
                "sources": [],
                "query": query,
                "search_results": []
            }
        
        # ?????????? context ? ?????
        context = "\n\n".join([
            f"??? {i+1}:\n{result['text'][:1000]}"
            for i, result in enumerate(search_results)
        ])
        
        sources = []
        for result in search_results:
            metadata = result.get('metadata', {})
            source = metadata.get('url', '') or metadata.get('title', '???? ??????')
            if source and source not in sources:
                sources.append(source)
        
        # ????? ???? ?? OpenAI
        rag_response = self.openai_client.generate_rag_response(
            query=normalized_query,
            context=context,
            sources=sources
        )
        
        return {
            **rag_response,
            "search_results": search_results[:3]  # ??? 3 ????? ????
        }
    
    def search_web(self, query: str, max_results: int = 3) -> List[Dict[str, Any]]:
        """??????? ?? ?? ?? Firecrawl"""
        return self.firecrawl_client.search_web(query, max_results)
    
    def ingest_url(self, url: str) -> Dict[str, Any]:
        """?????? URL ?? ?????? ????"""
        print(f"?? ?? ??? ?????? ??????: {url}")
        
        # ??????? ????? ?? Firecrawl
        scraped_data = self.firecrawl_client.scrape_url(url)
        
        if not scraped_data:
            return {
                "success": False,
                "message": "??? ?? ?????? ?????? URL"
            }
        
        # ?????????? ? ????? ?????
        content = scraped_data.get('content', '')
        normalized_content = self.normalize_text(content)
        chunks = self.chunk_text(normalized_content)
        
        print(f"?? ????? ??????? ????????: {len(chunks)}")
        
        # ?????? ?? Vector Store
        metadatas = [{
            'url': url,
            'title': scraped_data.get('title', ''),
            'chunk_index': i
        } for i in range(len(chunks))]
        
        self.vector_store.add_documents(chunks, metadatas)
        
        return {
            "success": True,
            "message": f"? {len(chunks)} ??? ?? ????? ?? ?????? ????? ??",
            "url": url,
            "title": scraped_data.get('title', ''),
            "chunks_count": len(chunks)
        }


# ????? ??????
rag_engine = RAGEngine()
